import { create } from "zustand";
import { 
  Node, 
  Edge, 
  Connection, 
  addEdge, 
  OnNodesChange, 
  OnEdgesChange,
  OnConnect,
  applyNodeChanges,
  applyEdgeChanges
} from "@xyflow/react";
import { v4 as uuidv4 } from "uuid";

export type NodeData = {
  label: string;
  type: string;
  config: Record<string, unknown>;
};

interface GraphState {
  nodes: Node<NodeData>[];
  edges: Edge[];
  selectedNode: string | null;
  
  // Node operations
  addNode: (type: string, position: { x: number; y: number }, config?: Record<string, unknown>) => void;
  addNodesWithEdges: (nodes: Array<{ type: string; position: { x: number; y: number }; config?: Record<string, unknown> }>, edges?: Array<{ source: number; target: number }>) => void;
  addOutputNode: () => void;
  updateNodeConfig: (nodeId: string, config: Record<string, unknown>) => void;
  deleteNode: (nodeId: string) => void;
  selectNode: (nodeId: string | null) => void;
  clearAll: () => void;
  
  // Edge operations
  onNodesChange: OnNodesChange<Node<NodeData>>;
  onEdgesChange: OnEdgesChange<Edge>;
  onConnect: OnConnect;
}

export const nodeDefaults: Record<string, Partial<NodeData>> = {
  pipeline: {
    label: "Pipeline",
    config: {
      pipelineId: "",
      pipelineName: "",
      remoteInference: false,
    },
  },
  videoInput: {
    label: "Video Input",
    config: {},
  },
  textPrompt: {
    label: "Text Prompt",
    config: { text: "", weight: 1.0 },
  },
  imageInput: {
    label: "Image Input",
    config: { path: "" },
  },
  parameters: {
    label: "Parameters",
    config: {},
  },
  brightness: {
    label: "Brightness",
    config: { value: 0 },
  },
  contrast: {
    label: "Contrast",
    config: { value: 1 },
  },
  blur: {
    label: "Blur",
    config: { radius: 5 },
  },
  mirror: {
    label: "Mirror",
    config: { mode: "horizontal" },
  },
  kaleido: {
    label: "Kaleido",
    config: { slices: 6, rotation: 0 },
  },
  blend: {
    label: "Blend",
    config: { mode: "add", opacity: 0.5 },
  },
  mask: {
    label: "Mask",
    config: { targetClass: "person" },
  },
  // Preprocessor-specific effects
  segmentation: {
    label: "Segmentation",
    config: { model: "sam", targetClass: "all" },
  },
  depthEstimation: {
    label: "Depth Estimation",
    config: { model: "depth-anything" },
  },
  backgroundRemoval: {
    label: "Background Removal",
    config: { model: "u2net" },
  },
  // Postprocessor-specific effects
  colorGrading: {
    label: "Color Grading",
    config: { temperature: 0, tint: 0, saturation: 0, contrast: 0 },
  },
  upscaling: {
    label: "Upscaling",
    config: { scale: 2, model: "realesrgan" },
  },
  denoising: {
    label: "Denoising",
    config: { strength: 0.5, method: "bm3d" },
  },
  styleTransfer: {
    label: "Style Transfer",
    config: { style: "anime", strength: 0.7 },
  },
  vignette: {
    label: "Vignette",
    config: { intensity: 0.5, smoothness: 0.5 },
  },
  pipelineOutput: {
    label: "Pipeline Output",
    config: { usage: "main" },
  },
  preprocessorOutput: {
    label: "Preprocessor Output",
    config: {},
  },
  postprocessorOutput: {
    label: "Postprocessor Output",
    config: {},
  },
  pluginConfig: {
    label: "Plugin Configuration",
    config: {
      pipelineId: "longlive",
      pipelineName: "My Plugin",
      pipelineDescription: "Generated by OpenScope",
      usage: "main", // main, preprocessor, postprocessor, all
      mode: "video", // text, video
      supportsPrompts: true,
      remoteInference: false,
    },
  },
  noteGuide: {
    label: "Note",
    config: {
      title: "Step 1",
      content: "1. Add instructions here.\n2. Edit in the properties panel.",
    },
  },
  lessonGettingStarted: {
    label: "Getting Started",
    config: {
      title: "1. Getting Started",
      content: `Welcome to OpenScope! ðŸŽ¯

OpenScope is a visual plugin builder for Daydream Scope that lets you create AI video processing pipelines without coding.

KEY CONCEPTS:
â€¢ Plugin Config - Defines your plugin's identity (name, ID, usage type)
â€¢ Inputs - Video, Text Prompts, or Image References
â€¢ Pipelines - AI models fetched from Scope server
â€¢ Preprocessors - Modify input before main pipeline
â€¢ Postprocessors - Modify output after main pipeline
â€¢ Effects - Visual enhancements like blur, kaleido, etc.

GETTING STARTED:
1. Create a Plugin Config node first
2. Choose your usage type (main/preprocessor/postprocessor)
3. Add input nodes based on your mode
4. Connect nodes by dragging from output to input handles
5. Toggle the code icon to see the Python code!

Press the Generate button in the header to create your plugin.`,
    },
  },
  lessonFirstProcessor: {
    label: "First Processor",
    config: {
      title: "2. Creating Your First Processor",
      content: `Let's create a simple postprocessor! ðŸš€

STEP 1: Set Usage
â€¢ In Plugin Config, set Usage to "postprocessor"

STEP 2: Add Video Input
â€¢ Expand Input category
â€¢ Drag "Video Input" to canvas

STEP 3: Add an Effect
â€¢ Expand Effects category (now visible!)
â€¢ Drag "Brightness" or "Blur" to canvas

STEP 4: Connect Nodes
â€¢ Click the right handle of Video Input
â€¢ Drag to the left handle of your effect

STEP 5: Add Output
â€¢ Expand Output category
â€¢ Drag "Postprocessor Output" to canvas
â€¢ Connect your effect to it

STEP 6: Generate!
â€¢ Click Generate to create your plugin

Congratulations! You've created your first processor!`,
    },
  },
  lessonNodeTypes: {
    label: "Node Types",
    config: {
      title: "3. Understanding Node Types",
      content: `Each node serves a purpose in your pipeline! ðŸ“¦

INPUT NODES:
â€¢ Video Input - Accepts video frames from camera/file
â€¢ Text Prompt - Text input with weight for AI models
â€¢ Image Input - Reference images for image-to-video
â€¢ Parameters - Key-value config for runtime

PIPELINE NODE:
â€¢ Fetches AI pipelines from your Scope server
â€¢ These are remote inference models
â€¢ Toggle code to see the Python implementation

PREPROCESSOR NODES:
â€¢ Segmentation - Object detection/masking
â€¢ Depth Estimation - Create depth maps
â€¢ Background Removal - Transparent backgrounds
â€¢ "New" - Create custom preprocessor

EFFECT NODES:
â€¢ Blur, Mirror, Kaleido, Vignette, etc.
â€¢ "New" - Create custom effect
â€¢ These modify frames visually

POSTPROCESSOR NODES:
â€¢ Color Grading - Professional color correction
â€¢ Upscaling - AI resolution increase
â€¢ Denoising - Remove noise
â€¢ Style Transfer - Apply artistic styles

OUTPUT NODES:
â€¢ Pipeline Output - Main AI result
â€¢ Preprocessor Output - Preprocessed data
â€¢ Postprocessor Output - Final processed result`,
    },
  },
  lessonPreprocessors: {
    label: "Preprocessors",
    config: {
      title: "4. Working with Preprocessors",
      content: `Preprocessors modify input before the main AI pipeline! ðŸ”§

WHEN TO USE:
â€¢ You need to prepare input for AI models
â€¢ Object masking before generation
â€¢ Depth guidance for structure
â€¢ Background removal for compositing

AVAILABLE PREPROCESSORS:

SEGMENTATION:
â€¢ Detects and masks objects in frames
â€¢ Use "person" class for portrait work
â€¢ Outputs masks for guidance

DEPTH ESTIMATION:
â€¢ Creates depth maps from 2D frames
â€¢ Great for VACE structural guidance
â€¢ Helps maintain 3D consistency

BACKGROUND REMOVAL:
â€¢ Removes background with alpha channel
â€¢ Outputs RGBA with transparency
â€¢ Perfect for compositing

CREATING CUSTOM:
â€¢ Use "New" in Preprocessor category
â€¢ Write Python code in code mode
â€¢ Process frames before main pipeline

TIPS:
â€¢ Preprocessors run BEFORE the main pipeline
â€¢ Connect output to main pipeline input
â€¢ Can chain multiple preprocessors`,
    },
  },
  lessonPostprocessors: {
    label: "Postprocessors",
    config: {
      title: "5. Working with Postprocessors",
      content: `Postprocessors enhance or modify the AI output! âœ¨

WHEN TO USE:
â€¢ After AI generation completes
â€¢ Color correction and grading
â€¢ Resolution enhancement
â€¢ Adding visual effects

AVAILABLE POSTPROCESSORS:

COLOR GRADING:
â€¢ Temperature, Tint, Saturation, Contrast
â€¢ Professional color correction
â€¢ Match footage styles

UPSCALING:
â€¢ AI-powered resolution increase
â€¢ 2x or 4x scale options
â€¢ Models: realesrgan, esrgan, swinir

DENOISING:
â€¢ Remove compression artifacts
â€¢ Clean up noisy outputs
â€¢ Strength adjustable

STYLE TRANSFER:
â€¢ Apply artistic styles
â€¢ Anime, oil, sketch, watercolor
â€¢ Adjustable strength

EFFECTS (also postprocessors):
â€¢ Brightness, Contrast, Blur
â€¢ Mirror, Kaleido, Vignette
â€¢ Many more visual effects

CREATING CUSTOM:
â€¢ Use "New" in Effects category
â€¢ Write Python code in code mode
â€¢ Process output frames

TIPS:
â€¢ Postprocessors run AFTER the main pipeline
â€¢ Can chain multiple postprocessors
â€¢ Effects category = visual postprocessors`,
    },
  },
};

export const useGraphStore = create<GraphState>((set, get) => ({
  nodes: [],
  edges: [],
  selectedNode: null,

  addNode: (type: string, position: { x: number; y: number }, config?: Record<string, unknown>) => {
    let defaults = nodeDefaults[type] || { label: type, config: {} };
    
    // Handle pipeline_ type (e.g., pipeline_animateDiff)
    if (type.startsWith("pipeline_")) {
      const pipelineId = type.replace("pipeline_", "");
      defaults = {
        label: pipelineId,
        config: { pipelineId, remoteInference: false },
      };
    }
    
    const newNode: Node<NodeData> = {
      id: uuidv4(),
      type: "scopeNode",
      position,
      selected: true,
      data: {
        label: defaults.label || type,
        type,
        config: { ...defaults.config, ...config },
      },
    };
    set({ nodes: [...get().nodes, newNode], selectedNode: newNode.id });
  },

  addNodesWithEdges: (nodes, edges) => {
    const NODE_WIDTH = 280;
    const NODE_HEIGHT = 150;
    const HORIZONTAL_GAP = 100;
    const VERTICAL_GAP = 80;

    const needsLayout = nodes.some(n => !n.position || n.position.x < 200);

    const newNodes: Node<NodeData>[] = nodes.map((n, index) => {
      const defaults = nodeDefaults[n.type] || { label: n.type, config: {} };
      
      let position = n.position;
      if (needsLayout && n.position) {
        position = {
          x: 50 + index * (NODE_WIDTH + HORIZONTAL_GAP),
          y: 50 + Math.floor(index / 3) * (NODE_HEIGHT + VERTICAL_GAP),
        };
      }

      return {
        id: uuidv4(),
        type: "scopeNode",
        position,
        selected: false,
        data: {
          label: defaults.label || n.type,
          type: n.type,
          config: { ...defaults.config, ...n.config },
        },
      };
    });

    let newEdges: Edge[] = [];
    
    const isPreprocessor = (type: string) => 
      ["segmentation", "depthEstimation", "backgroundRemoval"].includes(type);
    const isPostprocessor = (type: string) => 
      ["colorGrading", "upscaling", "denoising", "styleTransfer", "vignette", "kaleido", "blend", "mirror", "brightness", "contrast", "blur", "mask"].includes(type);
    const isInputNode = (type: string) => 
      ["videoInput", "textPrompt", "imageInput", "parameters"].includes(type);
    const isOutputNode = (type: string) => 
      ["pipelineOutput", "preprocessorOutput", "postprocessorOutput"].includes(type);
    const isMainPipeline = (type: string) => 
      type.startsWith("pipeline_") || type === "pipeline";
    
    if (edges && edges.length > 0) {
      newEdges = edges.map((e) => ({
        id: uuidv4(),
        source: newNodes[e.source]?.id,
        target: newNodes[e.target]?.id,
        type: "smoothstep",
        animated: true,
      }));
    } else if (newNodes.length > 1) {
      const pluginConfig = newNodes.find(n => n.data.type === "pluginConfig");
      const inputNodes = newNodes.filter(n => isInputNode(n.data.type));
      const preprocessorNodes = newNodes.filter(n => isPreprocessor(n.data.type));
      const postprocessorNodes = newNodes.filter(n => isPostprocessor(n.data.type));
      const mainPipelines = newNodes.filter(n => isMainPipeline(n.data.type));
      const preprocessorOutputs = newNodes.filter(n => n.data.type === "preprocessorOutput");
      const pipelineOutputs = newNodes.filter(n => n.data.type === "pipelineOutput");
      const postprocessorOutputs = newNodes.filter(n => n.data.type === "postprocessorOutput");
      const otherNodes = newNodes.filter(n => 
        !isInputNode(n.data.type) && 
        !isPreprocessor(n.data.type) && 
        !isPostprocessor(n.data.type) && 
        !isMainPipeline(n.data.type) &&
        !isOutputNode(n.data.type) &&
        n.data.type !== "pluginConfig"
      );

      let colIndex = 0;
      
      if (pluginConfig) {
        const pcIdx = newNodes.indexOf(pluginConfig);
        pluginConfig.position = { x: 50, y: 50 + colIndex * (NODE_HEIGHT + VERTICAL_GAP) };
        colIndex++;
      }

      inputNodes.forEach(node => {
        const idx = newNodes.indexOf(node);
        node.position = { x: 50 + (NODE_WIDTH + HORIZONTAL_GAP), y: 50 + colIndex * (NODE_HEIGHT + VERTICAL_GAP) };
        if (pluginConfig) {
          newEdges.push({
            id: uuidv4(),
            source: pluginConfig.id,
            target: node.id,
            type: "smoothstep",
            animated: true,
          });
        }
        colIndex++;
      });

      if (inputNodes.length === 0 && pluginConfig) {
        colIndex++;
      }

      preprocessorNodes.forEach(node => {
        const idx = newNodes.indexOf(node);
        node.position = { x: 50 + 2 * (NODE_WIDTH + HORIZONTAL_GAP), y: 50 + colIndex * (NODE_HEIGHT + VERTICAL_GAP) };
        const firstInput = inputNodes[0] || pluginConfig;
        if (firstInput) {
          newEdges.push({
            id: uuidv4(),
            source: firstInput.id,
            target: node.id,
            type: "smoothstep",
            animated: true,
          });
        }
      });

      preprocessorOutputs.forEach(node => {
        const idx = newNodes.indexOf(node);
        node.position = { x: 50 + 3 * (NODE_WIDTH + HORIZONTAL_GAP), y: 50 };
        preprocessorNodes.forEach(pre => {
          newEdges.push({
            id: uuidv4(),
            source: pre.id,
            target: node.id,
            type: "smoothstep",
            animated: true,
          });
        });
      });

      mainPipelines.forEach(node => {
        const idx = newNodes.indexOf(node);
        node.position = { x: 50 + 3 * (NODE_WIDTH + HORIZONTAL_GAP), y: 50 + (colIndex > 0 ? 1 : 0) * (NODE_HEIGHT + VERTICAL_GAP) };
        
        const firstPreprocessor = preprocessorNodes[0];
        const firstInput = inputNodes[0] || pluginConfig;
        
        if (firstPreprocessor) {
          newEdges.push({
            id: uuidv4(),
            source: firstPreprocessor.id,
            target: node.id,
            type: "smoothstep",
            animated: true,
          });
        } else if (firstInput) {
          newEdges.push({
            id: uuidv4(),
            source: firstInput.id,
            target: node.id,
            type: "smoothstep",
            animated: true,
          });
        }
      });

      pipelineOutputs.forEach(node => {
        const idx = newNodes.indexOf(node);
        node.position = { x: 50 + 4 * (NODE_WIDTH + HORIZONTAL_GAP), y: 50 };
        mainPipelines.forEach(pipeline => {
          newEdges.push({
            id: uuidv4(),
            source: pipeline.id,
            target: node.id,
            type: "smoothstep",
            animated: true,
          });
        });
      });

      postprocessorNodes.forEach(node => {
        const idx = newNodes.indexOf(node);
        node.position = { x: 50 + 4 * (NODE_WIDTH + HORIZONTAL_GAP), y: 50 + (pipelineOutputs.length > 0 ? 1 : 0) * (NODE_HEIGHT + VERTICAL_GAP) };
        
        mainPipelines.forEach(pipeline => {
          newEdges.push({
            id: uuidv4(),
            source: pipeline.id,
            target: node.id,
            type: "smoothstep",
            animated: true,
          });
        });
      });

      postprocessorOutputs.forEach(node => {
        const idx = newNodes.indexOf(node);
        node.position = { x: 50 + 5 * (NODE_WIDTH + HORIZONTAL_GAP), y: 50 };
        postprocessorNodes.forEach(post => {
          newEdges.push({
            id: uuidv4(),
            source: post.id,
            target: node.id,
            type: "smoothstep",
            animated: true,
          });
        });
      });

      otherNodes.forEach(node => {
        const idx = newNodes.indexOf(node);
        node.position = { x: 50 + 3 * (NODE_WIDTH + HORIZONTAL_GAP), y: 50 + colIndex * (NODE_HEIGHT + VERTICAL_GAP) };
        colIndex++;
      });
    }

    const pluginConfigNode = newNodes.find(n => n.data.type === "pluginConfig");
    const selectedId = pluginConfigNode?.id || newNodes[0]?.id;

    set({
      nodes: [...get().nodes, ...newNodes],
      edges: [...get().edges, ...newEdges],
      selectedNode: selectedId,
    });
  },

  updateNodeConfig: (nodeId: string, config: Record<string, unknown>) => {
    set({
      nodes: get().nodes.map((node) =>
        node.id === nodeId
          ? { ...node, data: { ...node.data, config: { ...node.data.config, ...config } } }
          : node
      ),
    });
  },

  deleteNode: (nodeId: string) => {
    set({
      nodes: get().nodes.filter((node) => node.id !== nodeId),
      edges: get().edges.filter(
        (edge) => edge.source !== nodeId && edge.target !== nodeId
      ),
      selectedNode: get().selectedNode === nodeId ? null : get().selectedNode,
    });
  },

  selectNode: (nodeId: string | null) => {
    set({ selectedNode: nodeId });
  },

  clearAll: () => {
    set({ nodes: [], edges: [], selectedNode: null });
  },

  addOutputNode: () => {
    const { nodes, edges } = get();
    if (nodes.length === 0) return;

    // Find the plugin config to determine usage
    const pluginConfig = nodes.find(n => n.data.type === "pluginConfig");
    const usage = (pluginConfig?.data?.config?.usage as string) || "main";

    // Determine output type based on usage
    let outputType = "pipelineOutput";
    if (usage === "preprocessor") outputType = "preprocessorOutput";
    else if (usage === "postprocessor") outputType = "postprocessorOutput";

    // Find the rightmost node to connect to
    let rightmostNode = nodes[0];
    let maxX = nodes[0].position.x;
    for (const node of nodes) {
      if (node.position.x > maxX) {
        maxX = node.position.x;
        rightmostNode = node;
      }
    }

    // Add output node
    const outputDefaults = nodeDefaults[outputType] || { label: outputType, config: {} };
    const outputNode: Node<NodeData> = {
      id: uuidv4(),
      type: "scopeNode",
      position: { x: maxX + 200, y: rightmostNode.position.y },
      selected: true,
      data: {
        label: outputDefaults.label || outputType,
        type: outputType,
        config: outputDefaults.config || {},
      },
    };

    // Create edge from rightmost node to output
    const newEdge: Edge = {
      id: uuidv4(),
      source: rightmostNode.id,
      target: outputNode.id,
      type: "smoothstep",
      animated: true,
    };

    set({
      nodes: [...nodes, outputNode],
      edges: [...edges, newEdge],
      selectedNode: outputNode.id,
    });
  },

  onNodesChange: (changes) => {
    set({ nodes: applyNodeChanges(changes, get().nodes) });
  },

  onEdgesChange: (changes) => {
    set({ edges: applyEdgeChanges(changes, get().edges) });
  },

  onConnect: (connection: Connection) => {
    set({ edges: addEdge({ ...connection, id: uuidv4() }, get().edges) });
  },
}));
